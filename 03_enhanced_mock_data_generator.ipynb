{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115f39d9-19ca-4a7c-bae9-d66b4d006a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# =============================================================================\n",
    "# UPI TRANSACTIONS CDC FEED PROJECT - ENHANCED MOCK DATA GENERATOR\n",
    "# =============================================================================\n",
    "# This notebook generates realistic mock data for UPI transactions with CDC operations\n",
    "# Purpose: Creates test data for development, testing, and demonstration of CDC streaming\n",
    "# Features: INSERT, UPDATE, DELETE operations with realistic UPI transaction patterns\n",
    "# Output: Continuous CDC operations to test real-time streaming pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bcc52b5-5d64-4e6d-a32c-d255b6066d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIBRARY IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "# Import required libraries for mock data generation and CDC operations\n",
    "# Purpose: Set up environment for generating realistic UPI transaction test data\n",
    "\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "# Configure Unity Catalog and target table for mock data generation\n",
    "# catalog_name: Unity Catalog for data governance\n",
    "# schema_name: Target schema for UPI transaction tables\n",
    "# raw_table: Target table for inserting mock transaction data\n",
    "\n",
    "catalog_name = \"`upi_catalog`\"\n",
    "schema_name = \"default\"\n",
    "raw_table = f\"{catalog_name}.{schema_name}.raw_upi_transactions_v1\"\n",
    "\n",
    "print(f\"Using catalog: {catalog_name}, schema: {schema_name}\")\n",
    "print(f\"Target table: {raw_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a1e4be-4629-4a73-8fac-4d2667d420c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mock data constants\n",
    "MERCHANTS = [\n",
    "    {\"merchant_id\": \"M001\", \"merchant_name\": \"Amazon India\", \"merchant_category\": \"E-commerce\"},\n",
    "    {\"merchant_id\": \"M002\", \"merchant_name\": \"Swiggy\", \"merchant_category\": \"Food Delivery\"},\n",
    "    {\"merchant_id\": \"M003\", \"merchant_name\": \"Uber\", \"merchant_category\": \"Transportation\"},\n",
    "    {\"merchant_id\": \"M004\", \"merchant_name\": \"Netflix\", \"merchant_category\": \"Entertainment\"},\n",
    "    {\"merchant_id\": \"M005\", \"merchant_name\": \"BigBasket\", \"merchant_category\": \"Grocery\"},\n",
    "    {\"merchant_id\": \"M006\", \"merchant_name\": \"Flipkart\", \"merchant_category\": \"E-commerce\"},\n",
    "    {\"merchant_id\": \"M007\", \"merchant_name\": \"Zomato\", \"merchant_category\": \"Food Delivery\"},\n",
    "    {\"merchant_id\": \"M008\", \"merchant_name\": \"Ola\", \"merchant_category\": \"Transportation\"}\n",
    "]\n",
    "\n",
    "UPI_IDS = [\"user123@paytm\", \"user456@phonepe\", \"user789@googlepay\", \"user101@amazonpay\", \"user202@mobikwik\"]\n",
    "CUSTOMER_IDS = [\"CUST001\", \"CUST002\", \"CUST003\", \"CUST004\", \"CUST005\"]\n",
    "PAYMENT_METHODS = [\"UPI\", \"QR Code\", \"Mobile Number\"]\n",
    "DEVICE_TYPES = [\"Mobile\", \"Tablet\"]\n",
    "OPERATING_SYSTEMS = [\"Android\", \"iOS\"]\n",
    "CITIES = [\"Mumbai\", \"Delhi\", \"Bangalore\", \"Chennai\", \"Kolkata\", \"Hyderabad\", \"Pune\", \"Ahmedabad\"]\n",
    "STATES = [\"Maharashtra\", \"Delhi\", \"Karnataka\", \"Tamil Nadu\", \"West Bengal\", \"Telangana\", \"Gujarat\"]\n",
    "AGE_GROUPS = [\"18-25\", \"26-35\", \"36-45\", \"46-55\", \"55+\"]\n",
    "GENDERS = [\"Male\", \"Female\", \"Other\"]\n",
    "\n",
    "# Transaction ID counter for unique IDs\n",
    "transaction_counter = 1\n",
    "\n",
    "def insert_new_transactions(num_transactions=5):\n",
    "    \"\"\"Insert new transactions using simple createDataFrame approach\"\"\"\n",
    "    global transaction_counter\n",
    "    try:\n",
    "        print(f\"INSERT: Adding {num_transactions} new transactions...\")\n",
    "        \n",
    "        # Generate transaction data as tuples (following your pattern)\n",
    "        transaction_data = []\n",
    "        \n",
    "        for i in range(num_transactions):\n",
    "            merchant = random.choice(MERCHANTS)\n",
    "            upi_id = random.choice(UPI_IDS)\n",
    "            customer_id = random.choice(CUSTOMER_IDS)\n",
    "            \n",
    "            # Generate realistic transaction amounts based on merchant category\n",
    "            if merchant[\"merchant_category\"] == \"E-commerce\":\n",
    "                amount = round(random.uniform(100, 5000), 2)\n",
    "            elif merchant[\"merchant_category\"] == \"Food Delivery\":\n",
    "                amount = round(random.uniform(50, 500), 2)\n",
    "            elif merchant[\"merchant_category\"] == \"Transportation\":\n",
    "                amount = round(random.uniform(20, 200), 2)\n",
    "            elif merchant[\"merchant_category\"] == \"Entertainment\":\n",
    "                amount = round(random.uniform(100, 1000), 2)\n",
    "            elif merchant[\"merchant_category\"] == \"Grocery\":\n",
    "                amount = round(random.uniform(200, 1000), 2)\n",
    "            else:\n",
    "                amount = round(random.uniform(50, 1000), 2)\n",
    "            \n",
    "            # Generate timestamp (recent for CDC testing)\n",
    "            transaction_time = datetime.now() - timedelta(minutes=random.randint(1, 60))\n",
    "            \n",
    "            # Generate transaction status\n",
    "            status_weights = {\"completed\": 0.75, \"failed\": 0.15, \"initiated\": 0.05, \"refunded\": 0.03, \"cancelled\": 0.02}\n",
    "            status = random.choices(list(status_weights.keys()), weights=list(status_weights.values()))[0]\n",
    "            \n",
    "            # Calculate processing fee and commission\n",
    "            processing_fee = round(amount * 0.005, 2)\n",
    "            commission = round(amount * 0.01, 2)\n",
    "            \n",
    "            # Generate unique transaction ID\n",
    "            transaction_id = f\"TXN_{datetime.now().strftime('%Y%m%d')}_{transaction_counter:06d}\"\n",
    "            transaction_counter += 1\n",
    "            \n",
    "            # Create tuple with all required fields (following your simple pattern)\n",
    "            transaction_tuple = (\n",
    "                transaction_id,\n",
    "                upi_id,\n",
    "                merchant[\"merchant_id\"],\n",
    "                merchant[\"merchant_name\"],\n",
    "                merchant[\"merchant_category\"],\n",
    "                float(amount),  # Use float/double as in your working example\n",
    "                \"INR\",\n",
    "                transaction_time,\n",
    "                status,\n",
    "                random.choice(PAYMENT_METHODS),\n",
    "                random.choice(DEVICE_TYPES),  # device_type\n",
    "                random.choice(OPERATING_SYSTEMS),  # device_os\n",
    "                f\"v{random.randint(1, 5)}.{random.randint(0, 9)}.{random.randint(0, 9)}\",  # app_version\n",
    "                round(random.uniform(8.0, 37.0), 6),  # latitude\n",
    "                round(random.uniform(68.0, 97.0), 6),  # longitude\n",
    "                random.choice(CITIES),  # city\n",
    "                random.choice(STATES),  # state\n",
    "                \"India\",  # country\n",
    "                customer_id,  # customer_id\n",
    "                random.choice(AGE_GROUPS),  # age_group\n",
    "                random.choice(GENDERS),  # gender\n",
    "                float(processing_fee),  # Use float/double as in your working example\n",
    "                float(commission),  # Use float/double as in your working example\n",
    "                datetime.now(),  # created_at\n",
    "                datetime.now()   # updated_at\n",
    "            )\n",
    "            \n",
    "            transaction_data.append(transaction_tuple)\n",
    "        \n",
    "        # Create DataFrame using simple createDataFrame (following your pattern)\n",
    "        transaction_df = spark.createDataFrame(transaction_data, [\n",
    "            \"transaction_id\", \"upi_id\", \"merchant_id\", \"merchant_name\", \"merchant_category\",\n",
    "            \"transaction_amount\", \"transaction_currency\", \"transaction_timestamp\", \"transaction_status\",\n",
    "            \"payment_method\", \"device_type\", \"device_os\", \"app_version\", \"latitude\", \"longitude\",\n",
    "            \"city\", \"state\", \"country\", \"customer_id\", \"age_group\", \"gender\",\n",
    "            \"processing_fee\", \"commission\", \"created_at\", \"updated_at\"\n",
    "        ])\n",
    "        \n",
    "        transaction_df.write.format(\"delta\").mode(\"append\").saveAsTable(raw_table)\n",
    "        \n",
    "        print(f\"INSERT: Successfully added {num_transactions} transactions\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"INSERT failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def update_existing_transactions(num_updates=3):\n",
    "    \"\"\"Update existing transactions using merge pattern with correct column names\"\"\"\n",
    "    try:\n",
    "        print(f\"UPDATE: Updating {num_updates} existing transactions...\")\n",
    "        \n",
    "        # Get existing transactions to update - FIXED COLUMN NAMES\n",
    "        existing_transactions = spark.sql(f\"\"\"\n",
    "            SELECT transaction_id, upi_id, merchant_id, merchant_name, merchant_category,\n",
    "                   transaction_amount, transaction_currency, transaction_timestamp,\n",
    "                   transaction_status, payment_method, device_type, device_os, app_version,\n",
    "                   latitude, longitude, city, state, country, customer_id, age_group, gender,\n",
    "                   processing_fee, commission, created_at\n",
    "            FROM {raw_table}\n",
    "            ORDER BY created_at DESC\n",
    "            LIMIT {num_updates}\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if len(existing_transactions) == 0:\n",
    "            print(\"No existing transactions to update\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare update data as tuples (following your pattern)\n",
    "        update_data = []\n",
    "        \n",
    "        for row in existing_transactions:\n",
    "            # Update the transaction status and amount\n",
    "            new_status = random.choice([\"completed\", \"failed\", \"refunded\"])\n",
    "            new_amount = round(row[\"transaction_amount\"] * random.uniform(0.8, 1.2), 2)\n",
    "            new_processing_fee = round(new_amount * 0.005, 2)\n",
    "            new_commission = round(new_amount * 0.01, 2)\n",
    "            \n",
    "            # Create tuple for update (following your pattern) - FIXED COLUMN NAMES\n",
    "            update_tuple = (\n",
    "                row[\"transaction_id\"],\n",
    "                row[\"upi_id\"],\n",
    "                row[\"merchant_id\"],\n",
    "                row[\"merchant_name\"],\n",
    "                row[\"merchant_category\"],\n",
    "                float(new_amount),  # Use float/double as in your working example\n",
    "                row[\"transaction_currency\"],\n",
    "                row[\"transaction_timestamp\"],\n",
    "                new_status,\n",
    "                row[\"payment_method\"],\n",
    "                row[\"device_type\"],  # FIXED: was device_info\n",
    "                row[\"device_os\"],    # FIXED: was device_info\n",
    "                row[\"app_version\"],  # FIXED: was device_info\n",
    "                row[\"latitude\"],     # FIXED: was location_info\n",
    "                row[\"longitude\"],    # FIXED: was location_info\n",
    "                row[\"city\"],         # FIXED: was location_info\n",
    "                row[\"state\"],        # FIXED: was location_info\n",
    "                row[\"country\"],      # FIXED: was location_info\n",
    "                row[\"customer_id\"],  # FIXED: was customer_info\n",
    "                row[\"age_group\"],    # FIXED: was customer_info\n",
    "                row[\"gender\"],       # FIXED: was customer_info\n",
    "                float(new_processing_fee),  # Use float/double as in your working example\n",
    "                float(new_commission),  # Use float/double as in your working example\n",
    "                row[\"created_at\"],\n",
    "                datetime.now()  # Update timestamp\n",
    "            )\n",
    "            \n",
    "            update_data.append(update_tuple)\n",
    "        \n",
    "        # Create DataFrame for updates (following your pattern) - FIXED COLUMN NAMES\n",
    "        update_df = spark.createDataFrame(update_data, [\n",
    "            \"transaction_id\", \"upi_id\", \"merchant_id\", \"merchant_name\", \"merchant_category\",\n",
    "            \"transaction_amount\", \"transaction_currency\", \"transaction_timestamp\", \"transaction_status\",\n",
    "            \"payment_method\", \"device_type\", \"device_os\", \"app_version\", \"latitude\", \"longitude\",\n",
    "            \"city\", \"state\", \"country\", \"customer_id\", \"age_group\", \"gender\",\n",
    "            \"processing_fee\", \"commission\", \"created_at\", \"updated_at\"\n",
    "        ])\n",
    "        \n",
    "        # Use merge pattern (following your approach)\n",
    "        delta_table = DeltaTable.forName(spark, raw_table)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            update_df.alias(\"source\"),\n",
    "            \"target.transaction_id = source.transaction_id\"\n",
    "        ).whenMatchedUpdateAll().execute()\n",
    "        \n",
    "        print(f\"UPDATE: Successfully updated {len(existing_transactions)} transactions\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"UPDATE failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def delete_transactions(num_deletes=2):\n",
    "    \"\"\"Delete some transactions\"\"\"\n",
    "    try:\n",
    "        print(f\"DELETE: Deleting {num_deletes} transactions...\")\n",
    "        \n",
    "        # Get transactions to delete\n",
    "        transactions_to_delete = spark.sql(f\"\"\"\n",
    "            SELECT transaction_id\n",
    "            FROM {raw_table}\n",
    "            ORDER BY created_at DESC\n",
    "            LIMIT {num_deletes}\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if len(transactions_to_delete) == 0:\n",
    "            print(\"No transactions to delete\")\n",
    "            return False\n",
    "        \n",
    "        # Delete transactions using DeltaTable\n",
    "        target_table = DeltaTable.forName(spark, raw_table)\n",
    "        \n",
    "        transaction_ids = [row[\"transaction_id\"] for row in transactions_to_delete]\n",
    "        \n",
    "        for transaction_id in transaction_ids:\n",
    "            target_table.delete(f\"transaction_id = '{transaction_id}'\")\n",
    "        \n",
    "        print(f\"DELETE: Successfully deleted {len(transaction_ids)} transactions\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DELETE failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"UPDATE and DELETE functions defined with fixed column names\")\n",
    "print(\"Transaction data generation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc996c3-c7b3-4423-a181-6648ac8b41b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def continuous_cdc_data_generation(duration_minutes=20):\n",
    "    \"\"\"Generate data continuously with CDC operations every 2 minutes\"\"\"\n",
    "    global transaction_counter\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting continuous CDC data generation for {duration_minutes} minutes\")\n",
    "        print(\"Operations will run every 2 minutes\")\n",
    "        print(\"Operations: INSERT (70%), UPDATE (20%), DELETE (10%)\")\n",
    "        \n",
    "        end_time = datetime.now() + timedelta(minutes=duration_minutes)\n",
    "        batch_count = 0\n",
    "        \n",
    "        while datetime.now() < end_time:\n",
    "            batch_count += 1\n",
    "            print(f\"Batch {batch_count} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            \n",
    "            # Determine operation type based on weights\n",
    "            operation_weights = {\"INSERT\": 0.7, \"UPDATE\": 0.2, \"DELETE\": 0.1}\n",
    "            operation = random.choices(list(operation_weights.keys()), weights=list(operation_weights.values()))[0]\n",
    "            \n",
    "            if operation == \"INSERT\":\n",
    "                success = insert_new_transactions(random.randint(3, 8))\n",
    "            elif operation == \"UPDATE\":\n",
    "                success = update_existing_transactions(random.randint(1, 4))\n",
    "            elif operation == \"DELETE\":\n",
    "                success = delete_transactions(random.randint(1, 2))\n",
    "            \n",
    "            if success:\n",
    "                print(f\"Batch {batch_count} completed successfully\")\n",
    "            else:\n",
    "                print(f\"Batch {batch_count} had issues\")\n",
    "            \n",
    "            # Wait for next batch (2 minutes)\n",
    "            if datetime.now() < end_time:\n",
    "                print(f\"Waiting 2 minutes for next batch...\")\n",
    "                time.sleep(120)  # 2 minutes\n",
    "        \n",
    "        print(f\"Continuous CDC data generation completed!\")\n",
    "        print(f\"Total batches processed: {batch_count}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Continuous data generation stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in continuous data generation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"Continuous CDC data generation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5174b9f-f653-4d23-a2b8-e602919f1c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up initial data\n",
    "print(\"Setting up initial data...\")\n",
    "success = insert_new_transactions(20)\n",
    "if success:\n",
    "    print(\"Initial data setup completed successfully!\")\n",
    "    print(\"You can now run: continuous_cdc_data_generation(duration_minutes=30)\")\n",
    "    continuous_cdc_data_generation(duration_minutes=20)\n",
    "else:\n",
    "    print(\"Initial data setup failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb48a5b-cb0c-46db-8a06-deac4c2965cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this if manually we want to run upate\n",
    "\n",
    "# update_existing_transactions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f144b3a-3f1a-4cee-b653-dacd0bbaaec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this if manually we want to run delete\n",
    "\n",
    "# delete_transactions(2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_enhanced_mock_data_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
